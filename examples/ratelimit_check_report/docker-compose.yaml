---
services:
  envoy:
    image: envoyproxy/envoy:v1.31-latest
    container_name: envoy
    depends_on:
      - limitador
      - upstream
      - llm-model
    command:
      - /usr/local/bin/envoy
      - --config-path
      - /etc/envoy.yaml
      - --log-level
      - info
      - --component-log-level
      - wasm:debug,http:debug,router:debug
      - --service-cluster
      - proxy
    expose:
      - "80"
      - "8001"
    ports:
      - "18000:80"
      - "18001:8001"
    volumes:
      - ./envoy.yaml:/etc/envoy.yaml
      - ../../target/wasm32-wasip1/debug/wasm_shim.wasm:/opt/kuadrant/wasm/wasm_shim.wasm
  limitador:
    image: ${LIMITADOR_IMAGE:-quay.io/kuadrant/limitador:v2.2.0}
    container_name: limitador
    command:
      - limitador-server
      - --rls-ip
      - 0.0.0.0
      - --rls-port
      - "8081"
      - --http-ip
      - 0.0.0.0
      - --http-port
      - "8080"
      - -vvv
      - --grpc-reflection-service
      - --rate-limit-headers
      - DRAFT_VERSION_03
      - /opt/kuadrant/limits/limits.yaml
      - memory
    expose:
      - "8080"
      - "8081"
    ports:
      - "18080:8080"
      - "18081:8081"
    volumes:
      - ./limits.yaml:/opt/kuadrant/limits/limits.yaml
  upstream:
    image: quay.io/openshift-logging/alpine-socat:1.8.0.0
    container_name: upstream
    command: "-d -d -v -d TCP-LISTEN:80,reuseaddr,fork TCP:llm-model:8000"
    expose:
      - "80"
    restart: unless-stopped
  llm-model:
    image: ghcr.io/llm-d/llm-d-inference-sim:v0.1.1
    container_name: llm-model
    command:
      - --model
      - meta-llama/Llama-3.1-8B-Instruct
      - --port
      - "8000"
    ports:
      - "8000:8000"
    expose:
      - "8000"
  start_services:
    image: alpine
    depends_on:
      - envoy
    command: >
      /bin/sh -c "
      while ! nc -z envoy 80;
      do
      echo sleeping;
      sleep 1;
      done;
      echo Connected!
      "
