---
services:
  envoy:
    image: envoyproxy/envoy:v1.31-latest
    container_name: envoy
    depends_on:
    - limitador
    - upstream
    - llm-model
    command:
    - /usr/local/bin/envoy
    - --config-path
    - /etc/envoy.yaml
    - --log-level
    - info
    - --component-log-level
    - wasm:debug,http:debug,router:debug
    - --service-cluster
    - proxy
    expose:
    - "80"
    - "8001"
    ports:
    - "18000:80"
    - "18001:8001"
    volumes:
    - ./envoy.yaml:/etc/envoy.yaml
    - ../../target/wasm32-unknown-unknown/debug/wasm_shim.wasm:/opt/kuadrant/wasm/wasm_shim.wasm
  limitador:
    image: ${LIMITADOR_IMAGE:-quay.io/kuadrant/limitador:latest}
    container_name: limitador
    command:
      - limitador-server
      - --rls-ip
      - 0.0.0.0
      - --rls-port
      - "8081"
      - --http-ip
      - 0.0.0.0
      - --http-port
      - "8080"
      - -vvv
      - --grpc-reflection-service
      - --rate-limit-headers
      - DRAFT_VERSION_03
      - /opt/kuadrant/limits/limits.yaml
      - memory
    expose:
      - "8080"
      - "8081"
    ports:
      - "18080:8080"
      - "18081:8081"
    volumes:
      - ./limits.yaml:/opt/kuadrant/limits/limits.yaml
  upstream:
    image: quay.io/openshift-logging/alpine-socat:1.8.0.0
    container_name: upstream
    command: "-d -d -v -d TCP-LISTEN:80,reuseaddr,fork TCP:llm-model:8000"
    expose:
      - "80"
    restart: unless-stopped
  llm-model:
    image: ghcr.io/llm-d/llm-d-inference-sim:v0.1.1
    container_name: llm-model
    command:
    - --model
    - meta-llama/Llama-3.1-8B-Instruct
    - --port
    - "8000"
    ports:
    - "8000:8000"
    expose:
    - "8000"
  start_services:
    image: alpine
    depends_on:
    - envoy
    command: >
      /bin/sh -c "
      while ! nc -z envoy 80;
      do
      echo sleeping;
      sleep 1;
      done;
      echo Connected!
      "
